{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027004c8",
   "metadata": {},
   "source": [
    "### Libraries and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836387e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "\n",
    "from models.MultiStage_Composite_V4 import CompositeModel\n",
    "from util import preprocess_image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eb826",
   "metadata": {},
   "source": [
    "### Model and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a46741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Composite model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "images_folder = os.path.join(\"..\", \"..\", \"CV_data\", \"miniImageNet\")  # Path to images\n",
    "qwen_embeddings_path = os.path.join(\"..\", \"..\", \"CV_data\", \"separated_embeddings\", \"qwen_384_test\")  # Path to Qwen embeddings\n",
    "test_images_csv = os.path.join(\"..\", \"assets\", \"test_images.csv\")  # CSV file with test image filenames and labels\n",
    "composite_model_state_path = os.path.join(\"..\", \"models\", \"V5_finetuning_adapter\", \"best_model_epoch_2_loss_2.0951.pth\")  # Path to the trained composite model\n",
    "\n",
    "# Model\n",
    "model = CompositeModel().to(device).half()\n",
    "composite_model_state_dict = torch.load(composite_model_state_path, weights_only=False)['model_state_dict']\n",
    "model.load_state_dict(composite_model_state_dict)\n",
    "print(\"[INFO] Composite model loaded.\")\n",
    "\n",
    "# Image settings\n",
    "image_size = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f0f8c",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86edf940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LOOP] Processing images: 100%|██████████| 9001/9001 [02:54<00:00, 51.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Average Loss: 2.3568\n",
      "[INFO] Standard Deviation: 0.5679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Image names list from CSV\n",
    "df = pd.read_csv(test_images_csv)\n",
    "image_names = df['filename'].tolist()\n",
    "\n",
    "# Loss history\n",
    "loss_history = []\n",
    "\n",
    "# Loop through images and extract embeddings\n",
    "for img_name in tqdm(image_names, desc=\"[LOOP] Processing images\"):\n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        img_path = os.path.join(images_folder, img_name)\n",
    "        image = preprocess_image(img_path, image_size)\n",
    "        transform = transforms.Compose([transforms.ToTensor(),])\n",
    "        resnet_inputs = transform(image).unsqueeze(0).to(device, torch.float16)\n",
    "\n",
    "        # Extract Qwen embeddings\n",
    "        with torch.no_grad():\n",
    "            composite_model_output = model(resnet_inputs)\n",
    "\n",
    "        # load qwen emb from qwen_embeddings_path\n",
    "        qwen_emb = torch.load(os.path.join(qwen_embeddings_path, f\"{os.path.splitext(img_name)[0]}.pt\")).to(device, torch.float16)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(composite_model_output, qwen_emb)\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        # Free up memory\n",
    "        del composite_model_output, qwen_emb, resnet_inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error processing {img_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Print average loss and standard deviation\n",
    "if loss_history:\n",
    "    avg_loss = np.nanmean(loss_history)\n",
    "    print(f\"\\n[INFO] Average Loss: {avg_loss:.4f}\")\n",
    "    std_loss = np.nanstd(loss_history)\n",
    "    print(f\"[INFO] Standard Deviation: {std_loss:.4f}\")\n",
    "else:\n",
    "    print(\"[INFO] No losses recorded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
