{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04919c26",
   "metadata": {},
   "source": [
    "### Libraries and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c2181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "import torch\n",
    "import os\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "from util import preprocess_image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1e072",
   "metadata": {},
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6b8f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading Qwen processor...\n",
      "[INFO] Qwen processor loaded.\n",
      "\n",
      "[INFO] Loading Qwen model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf4f9b8b1a1497287ef829247b0b800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Qwen model loaded.\n"
     ]
    }
   ],
   "source": [
    "qwen_model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "qwen_processor_folder = os.path.join(\"..\", \"assets\", \"qwen_processor\")\n",
    "\n",
    "print(\"[INFO] Loading Qwen processor...\")\n",
    "qwen_processor = AutoProcessor.from_pretrained(qwen_processor_folder, trust_remote_code=True, use_fast=True, verbose=False)\n",
    "print(\"[INFO] Qwen processor loaded.\\n\")\n",
    "\n",
    "print(\"[INFO] Loading Qwen model...\")\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    qwen_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "qwen_model.to(device)\n",
    "print(\"[INFO] Qwen model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f28ef",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae1108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating...\n",
      "\n",
      "--- Output ---\n",
      "The image depicts an artificial satellite in orbit around Earth. The satellite has a rectangular shape with multiple panels, likely solar panels, attached to its exterior. These panels are designed to capture sunlight and convert it into electrical energy, which powers the satellite's systems. The satellite is positioned above the Earth's surface, showing a clear view of the planet's curvature and the blue oceans below. The background features a starry night sky, indicating that the satellite is in space. The overall scene suggests a typical scenario for a communication or observation satellite in operation.\n"
     ]
    }
   ],
   "source": [
    "# Image setup\n",
    "image_size = 384\n",
    "image_path = os.path.join(\"..\", \"test_images\", \"satellite.jpg\")\n",
    "image = preprocess_image(image_path, image_size)\n",
    "\n",
    "\n",
    "# Prepare inputs\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = qwen_processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "inputs = qwen_processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Inference\n",
    "print(\"[INFO] Generating...\")\n",
    "with torch.no_grad():\n",
    "    outputs = qwen_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs['input_ids'], outputs)\n",
    "]\n",
    "output_text = qwen_processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(\"\\n--- Output ---\")\n",
    "print(output_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
