{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04919c26",
   "metadata": {},
   "source": [
    "### Libraries and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3c2181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1e072",
   "metadata": {},
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6b8f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qwen processor loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8acb15457464ec68c6d5cc79540afd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qwen model loaded.\n"
     ]
    }
   ],
   "source": [
    "qwen_model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(qwen_model_name, trust_remote_code=True)\n",
    "print(\"✅ Qwen processor loaded.\")\n",
    "\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    qwen_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "qwen_model.to(device)\n",
    "print(\"✅ Qwen model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f28ef",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae1108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n",
      "\n",
      "--- Output ---\n",
      "The image depicts an artificial satellite in orbit around Earth. The satellite has a rectangular shape with multiple panels, likely solar panels, attached to its exterior. These panels are designed to capture sunlight and convert it into electrical energy, which powers the satellite's systems. The satellite is positioned above the Earth's surface, showing a clear view of the planet's curvature and the blue oceans below. The background features a starry night sky, indicating that the satellite is in space. The overall scene suggests a typical scenario for a communication or observation satellite in operation.\n"
     ]
    }
   ],
   "source": [
    "# Image setup\n",
    "image_path = os.path.join(\"..\", \"..\", \"test_images\", \"5.jpg\")\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "W, H = image.size\n",
    "scale = 384 / min(W, H)\n",
    "new_W = int(W * scale)\n",
    "new_H = int(H * scale)\n",
    "image = image.resize((new_W, new_H), resample=Image.BICUBIC)\n",
    "W, H = image.size\n",
    "crop_size = min(W, H)\n",
    "left = (W - crop_size) // 2\n",
    "top = (H - crop_size) // 2\n",
    "image = image.crop((left, top, left + crop_size, top + crop_size))\n",
    "\n",
    "\n",
    "# Prepare inputs\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = qwen_processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "inputs = qwen_processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Inference\n",
    "print(\"Generating...\")\n",
    "with torch.no_grad():\n",
    "    outputs = qwen_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs['input_ids'], outputs)\n",
    "]\n",
    "output_text = qwen_processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(\"\\n--- Output ---\")\n",
    "print(output_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
