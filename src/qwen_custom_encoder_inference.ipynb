{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a2784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, AutoImageProcessor, ResNetModel\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VisionTransformerPretrainedModel\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca724d09",
   "metadata": {},
   "source": [
    "### Adaptive Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c79b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStageAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that takes embeddings from 4 stages of ResNet and converts them into embeddings for Qwen.\n",
    "    It uses 1x1 convolutions to project each stage to the desired output dimension, upsamples them to the\n",
    "    same spatial size, concatenates them, and applies a fusion network. Finally, it flattens the spatial dimensions\n",
    "    and applies a linear interpolation to get the target sequence length. A Layer Normalization is applied at the end.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stage_channels=[256, 512, 1024, 2048], out_dim=2048, hidden_multiplier=2):\n",
    "        \"\"\"\n",
    "        Constructor for MultiStageAdapter.\n",
    "\n",
    "        Args:\n",
    "            stage_channels (list): List of channel dimensions for each ResNet stage.\n",
    "            out_dim (int): Desired output dimension for Qwen embeddings.\n",
    "            hidden_multiplier (int): Multiplier for the hidden dimension in the fusion network.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1x1 convolutions to project each stage to out_dim\n",
    "        self.projections = nn.ModuleList([\n",
    "            nn.Conv2d(c, out_dim, kernel_size=1) for c in stage_channels\n",
    "        ])\n",
    "\n",
    "        # Fusion network, a small MLP with GELU activation between two linear layers implemented as 1x1 convolutions\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(out_dim * len(stage_channels), out_dim * hidden_multiplier, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_dim * hidden_multiplier, out_dim, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        # Final Layer Normalization\n",
    "        self.final_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "\n",
    "    def forward(self, stage0, stage1, stage2, stage3, target_seq_len=196):\n",
    "        \"\"\"\n",
    "        Forward pass for MultiStageAdapter.\n",
    "\n",
    "        Args:\n",
    "            stage0, stage1, stage2, stage3 (torch.Tensor): Feature maps from the 4 ResNet stages.\n",
    "            target_seq_len (int): Desired sequence length for the output embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get spatial dimensions from the last stage\n",
    "        B, _, Ht, Wt = stage3.shape\n",
    "\n",
    "        # Project each stage to out_dim and upsample to the size of the last stage\n",
    "        proj_feats = []\n",
    "        for feat, proj in zip([stage0, stage1, stage2, stage3], self.projections):\n",
    "            x = proj(feat)\n",
    "            x = F.interpolate(x, size=(Ht, Wt), mode='bilinear', align_corners=False)\n",
    "            proj_feats.append(x)\n",
    "\n",
    "        # Concatenate along the channel dimension and apply fusion network\n",
    "        fused = torch.cat(proj_feats, dim=1)  # (B, out_dim*4, Ht, Wt)\n",
    "        fused = self.fusion(fused)           # (B, out_dim, Ht, Wt)\n",
    "\n",
    "        # Flatten spatial dimensions, interpolate to target sequence length and permute to (B, L, C)\n",
    "        seq = fused.flatten(2)               # (B, out_dim, Ht*Wt)\n",
    "        seq = F.interpolate(seq, size=target_seq_len, mode='linear', align_corners=False)\n",
    "        seq = seq.permute(0, 2, 1)           # (B, L, C)\n",
    "        \n",
    "        # Apply final Layer Normalization\n",
    "        seq = self.final_norm(seq)\n",
    "        \n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37264255",
   "metadata": {},
   "source": [
    "### Composite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b70501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Composite model that integrates ResNet for image feature extraction and MultiStageAdapter\n",
    "    to convert these features into embeddings suitable for Qwen.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for CompositeModel.\n",
    "        Initializes the ResNet model and the MultiStageAdapter.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.resnet = ResNetModel.from_pretrained(\"microsoft/resnet-50\")\n",
    "        self.adapter = MultiStageAdapter()\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values, target_seq_len=196):\n",
    "        \"\"\"\n",
    "        Forward pass for CompositeModel.\n",
    "        It extracts features from ResNet and passes them through the MultiStageAdapter.\n",
    "\n",
    "        Args:\n",
    "            pixel_values (torch.Tensor): Input image tensor.\n",
    "            target_seq_len (int): Desired sequence length for the output embeddings.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output embeddings suitable for Qwen.\n",
    "        \"\"\"\n",
    "\n",
    "        intermediate_outputs = {}\n",
    "\n",
    "        # Register hooks to capture outputs from each ResNet stage\n",
    "        def get_hook(idx):\n",
    "            def hook(module, input, output):\n",
    "                intermediate_outputs[f\"stage_{idx}\"] = output\n",
    "            return hook\n",
    "\n",
    "        # Register hooks for each stage\n",
    "        hooks = []\n",
    "        for idx, stage in enumerate(self.resnet.encoder.stages):\n",
    "            hooks.append(stage.register_forward_hook(get_hook(idx)))\n",
    "\n",
    "        # Forward pass through ResNet to get intermediate features\n",
    "        intermediate_outputs.clear()\n",
    "        _ = self.resnet(pixel_values)\n",
    "\n",
    "        # Remove hooks\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "        # Extract features from each stage\n",
    "        stage0, stage1, stage2, stage3 = (\n",
    "            intermediate_outputs[\"stage_0\"],\n",
    "            intermediate_outputs[\"stage_1\"],\n",
    "            intermediate_outputs[\"stage_2\"],\n",
    "            intermediate_outputs[\"stage_3\"],\n",
    "        )\n",
    "\n",
    "        # Pass the features through the MultiStageAdapter\n",
    "        projected = self.adapter(stage0, stage1, stage2, stage3, target_seq_len)\n",
    "        \n",
    "        return projected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ec655",
   "metadata": {},
   "source": [
    "### Custom visual encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffdb90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomQwenVisualEncoder(Qwen2_5_VisionTransformerPretrainedModel):\n",
    "    \"\"\"\n",
    "    Custom Visual Encoder for Qwen that can be modified as needed.\n",
    "    Replace the forward method with an empty pass-through logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        \n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for CustomQwenVisualEncoder.\n",
    "        Currently, it acts as a pass-through, returning the input hidden_states unchanged.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): Input tensor (typically the preprocessed image from the processor).\n",
    "            grid_thw (torch.Tensor): Information about the grid of patches (may be used for final dimension).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Unchanged input hidden_states.\n",
    "        \"\"\"\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730a596",
   "metadata": {},
   "source": [
    "### Models settings and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b04f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading processors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Qwen tokenizer loaded.\n",
      "\n",
      "[INFO] Loading Qwen model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0211f198e2694217b990fd4606afce91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Qwen model loaded.\n",
      "\n",
      "[INFO] Qwen visual encoder replaced with custom encoder.\n"
     ]
    }
   ],
   "source": [
    "# Model names and paths\n",
    "qwen_model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "resnet_model_name = \"microsoft/resnet-50\"\n",
    "composite_model_state_path = os.path.join(\"..\", \"models\", \"layernorm_best_model.pth\")\n",
    "\n",
    "\n",
    "# Processor Qwen\n",
    "print(\"[INFO] Loading processors...\")\n",
    "tokenizer = AutoProcessor.from_pretrained(qwen_model_name, trust_remote_code=True).tokenizer\n",
    "print(\"[INFO] Qwen tokenizer loaded.\")\n",
    "\n",
    "\n",
    "# Qwen Model\n",
    "print(\"\\n[INFO] Loading Qwen model...\")\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    qwen_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "qwen_model.to(device)\n",
    "print(\"[INFO] Qwen model loaded.\")\n",
    "\n",
    "\n",
    "# Composite model\n",
    "composite_model = CompositeModel().to(device).half()\n",
    "composite_model.load_state_dict(torch.load(composite_model_state_path))\n",
    "\n",
    "\n",
    "# Custom visual encoder\n",
    "custom_visual_encoder = CustomQwenVisualEncoder(output_dim=qwen_model.config.hidden_size, config=qwen_model.config.vision_config)\n",
    "custom_visual_encoder.eval()\n",
    "qwen_model.model.visual = custom_visual_encoder\n",
    "print(\"\\n[INFO] Qwen visual encoder replaced with custom encoder.\")\n",
    "\n",
    "\n",
    "# Predefined inputs for Qwen model\n",
    "qwen_inputs = torch.load(os.path.join(\"..\", \"assets\", \"qwen_inputs.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b45b2",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "702045fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating...\n",
      "\n",
      "--- Output ---\n",
      "The image depicts a large, rectangular structure that appears to be a solar panel array. The panels are arranged in rows and columns, forming a grid-like pattern. Each panel is covered with photovoltaic cells designed to convert sunlight into electricity. The panels are mounted on a metal frame, which is supported by multiple vertical poles. The background shows a clear sky with no visible clouds, indicating good weather conditions for solar energy production.\n",
      "\n",
      "The solar panels are positioned at an angle to maximize their exposure to the sun's rays, which is typical for solar farms or large-scale solar power installations. The ground around the panels is relatively flat, suggesting that the\n"
     ]
    }
   ],
   "source": [
    "# Image setup\n",
    "image_path = os.path.join(\"..\", \"test_images\", \"satellite.jpg\")\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "W, H = image.size\n",
    "scale = 384 / min(W, H)\n",
    "new_W = int(W * scale)\n",
    "new_H = int(H * scale)\n",
    "image = image.resize((new_W, new_H), resample=Image.BICUBIC)\n",
    "W, H = image.size\n",
    "crop_size = min(W, H)\n",
    "left = (W - crop_size) // 2\n",
    "top = (H - crop_size) // 2\n",
    "image = image.crop((left, top, left + crop_size, top + crop_size))\n",
    "\n",
    "\n",
    "# Custom pixel values\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((384, 384)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "resnet_inputs = transform(image).unsqueeze(0).to(device, torch.float16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    custom_pixel_values = composite_model(resnet_inputs)\n",
    "    custom_pixel_values = custom_pixel_values.squeeze(0)\n",
    "\n",
    "\n",
    "# Generate inputs for Qwen from saved file and replace pixel_values\n",
    "qwen_inputs['pixel_values'] = custom_pixel_values\n",
    "\n",
    "\n",
    "# Inference\n",
    "print(\"[INFO] Generating...\")\n",
    "with torch.no_grad():\n",
    "    outputs = qwen_model.generate(\n",
    "        **qwen_inputs,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(qwen_inputs['input_ids'], outputs)\n",
    "]\n",
    "output_text = tokenizer.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(\"\\n--- Output ---\")\n",
    "print(output_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
