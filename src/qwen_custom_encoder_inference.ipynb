{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a2784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VisionTransformerPretrainedModel\n",
    "import os\n",
    "\n",
    "from models.MultiStage_FtResnet import CompositeModel\n",
    "from util import preprocess_image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ec655",
   "metadata": {},
   "source": [
    "### Custom visual encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdb90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomQwenVisualEncoder(Qwen2_5_VisionTransformerPretrainedModel):\n",
    "    \"\"\"\n",
    "    Custom Visual Encoder for Qwen that can be modified as needed.\n",
    "    Replace the forward method with an empty pass-through logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        \n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for CustomQwenVisualEncoder.\n",
    "        Currently, it acts as a pass-through, returning the input hidden_states unchanged.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): Input tensor (typically the preprocessed image from the processor).\n",
    "            grid_thw (torch.Tensor): Information about the grid of patches (may be used for final dimension).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Unchanged input hidden_states.\n",
    "        \"\"\"\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730a596",
   "metadata": {},
   "source": [
    "### Models settings and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b04f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading Qwen tokenizer...\n",
      "[INFO] Qwen tokenizer loaded.\n",
      "\n",
      "[INFO] Loading Qwen model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3657f5a2d60d473b85353e8a8f410005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Qwen model loaded.\n",
      "\n",
      "[INFO] Qwen visual encoder replaced with custom encoder.\n"
     ]
    }
   ],
   "source": [
    "# Model name and paths\n",
    "qwen_model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "qwen_processor_folder = os.path.join(\"..\", \"assets\", \"qwen_processor\")\n",
    "\n",
    "\n",
    "# Composite model state path\n",
    "composite_model_state_path = os.path.join(\"..\", \"models\", \"layernorm_best_model.pth\")\n",
    "composite_model_state_dict = torch.load(composite_model_state_path, weights_only=False)\n",
    "#composite_model_state_path = os.path.join(\"..\", \"models\", \"composite_model_checkpoints\", \"best_model_epoch_16_loss_2.6507.pth\")\n",
    "#composite_model_state_dict = torch.load(composite_model_state_path, weights_only=False)['model_state_dict']\n",
    "\n",
    "\n",
    "# Processor Qwen\n",
    "print(\"[INFO] Loading Qwen tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(qwen_processor_folder, trust_remote_code=True)\n",
    "print(\"[INFO] Qwen tokenizer loaded.\")\n",
    "\n",
    "\n",
    "# Qwen Model\n",
    "print(\"\\n[INFO] Loading Qwen model...\")\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    qwen_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "qwen_model.to(device)\n",
    "print(\"[INFO] Qwen model loaded.\")\n",
    "\n",
    "\n",
    "# Composite model\n",
    "composite_model = CompositeModel().to(device).half()\n",
    "composite_model.load_state_dict(composite_model_state_dict)\n",
    "\n",
    "\n",
    "# Custom visual encoder\n",
    "custom_visual_encoder = CustomQwenVisualEncoder(config=qwen_model.config.vision_config)\n",
    "custom_visual_encoder.eval()\n",
    "qwen_model.model.visual = custom_visual_encoder\n",
    "print(\"\\n[INFO] Qwen visual encoder replaced with custom encoder.\")\n",
    "\n",
    "\n",
    "# Predefined inputs for Qwen model\n",
    "qwen_inputs = torch.load(os.path.join(\"..\", \"assets\", \"qwen_inputs.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b45b2",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702045fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating...\n",
      "\n",
      "--- Output ---\n",
      "The image depicts a large, rectangular structure that appears to be a solar panel array. The panels are arranged in rows and columns, forming a grid-like pattern. Each panel is covered with photovoltaic cells designed to convert sunlight into electricity. The panels are mounted on a metal frame, which is supported by multiple vertical poles. The background shows a clear sky with no visible clouds, indicating good weather conditions for solar energy production.\n",
      "\n",
      "The solar panels are positioned at an angle to maximize their exposure to the sun's rays, which is typical for solar farms or large-scale solar power installations. The ground around the panels is relatively flat, suggesting that the\n"
     ]
    }
   ],
   "source": [
    "# Image setup\n",
    "image_size = 384\n",
    "image_path = os.path.join(\"..\", \"test_images\", \"satellite.jpg\")\n",
    "image = preprocess_image(image_path, image_size)\n",
    "\n",
    "\n",
    "# Custom pixel values\n",
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "resnet_inputs = transform(image).unsqueeze(0).to(device, torch.float16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    custom_pixel_values = composite_model(resnet_inputs)\n",
    "    custom_pixel_values = custom_pixel_values.squeeze(0)\n",
    "\n",
    "\n",
    "# Generate inputs for Qwen from saved file and replace pixel_values\n",
    "qwen_inputs['pixel_values'] = custom_pixel_values\n",
    "\n",
    "\n",
    "# Inference\n",
    "print(\"[INFO] Generating...\")\n",
    "with torch.no_grad():\n",
    "    outputs = qwen_model.generate(\n",
    "        **qwen_inputs,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(qwen_inputs['input_ids'], outputs)\n",
    "]\n",
    "output_text = tokenizer.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(\"\\n--- Output ---\")\n",
    "print(output_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
