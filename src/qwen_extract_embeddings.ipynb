{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e65be4",
   "metadata": {},
   "source": [
    "### Libraries and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d0bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "import traceback\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from util import preprocess_image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6753261",
   "metadata": {},
   "source": [
    "### Qwen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "qwen_processor_folder = os.path.join(\"..\", \"assets\", \"qwen_processor\")\n",
    "\n",
    "print(\"[INFO] Loading Qwen processor...\")\n",
    "qwen_processor = AutoProcessor.from_pretrained(qwen_processor_folder, trust_remote_code=True, use_fast=True, verbose=False)\n",
    "print(\"[INFO] Qwen processor loaded.\\n\")\n",
    "\n",
    "print(\"[INFO] Loading Qwen model...\")\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    qwen_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "qwen_model = qwen_model.visual\n",
    "qwen_model.to(device)\n",
    "print(\"[INFO] Qwen model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8dd53",
   "metadata": {},
   "source": [
    "## Extract image's embedding with Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512effda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qwen_embedding(image_path, img_size=384):\n",
    "    \"\"\"\n",
    "    Extract embedding from an image using the Qwen model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        img_size (int): Size to which the image will be resized (default is 384).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Extracted embedding tensor, or None if extraction fails.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the model and processor are loaded\n",
    "    if not qwen_model or not qwen_processor:\n",
    "        print(\"[ERROR] Qwen model or processor not loaded.\")\n",
    "        return None\n",
    "\n",
    "    # Initialize variables\n",
    "    image = None\n",
    "    inputs_payload_cpu = None \n",
    "    pixel_values = None\n",
    "    image_grid_thw = None\n",
    "    model_output_tensor = None\n",
    "    embedding_cpu = None\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    try:\n",
    "        image = preprocess_image(image_path, img_size)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] File {image_path} not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Unable to open image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Prepare the input for the model\n",
    "        inputs_payload_cpu = qwen_processor.image_processor(images=[image], return_tensors=\"pt\")\n",
    "        del image\n",
    "\n",
    "        pixel_values = inputs_payload_cpu['pixel_values'].to(device)\n",
    "        image_grid_thw = inputs_payload_cpu['image_grid_thw'].to(device)\n",
    "        del inputs_payload_cpu\n",
    "\n",
    "        # Extract embedding using the Qwen model\n",
    "        with torch.no_grad():\n",
    "            model_output_tensor = qwen_model(pixel_values, grid_thw=image_grid_thw)\n",
    "\n",
    "            if model_output_tensor is not None and model_output_tensor.nelement() > 0:\n",
    "                embedding_cpu = model_output_tensor.cpu()\n",
    "            else:\n",
    "                print(f\"[ERROR] No output or empty output from model for image {image_path}.\")\n",
    "                embedding_cpu = None\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"[ERROR] Runtime error during extraction for {image_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        embedding_cpu = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Unexpected error during extraction for {image_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        embedding_cpu = None\n",
    "\n",
    "    finally:\n",
    "        # Free up memory\n",
    "        del pixel_values\n",
    "        del image_grid_thw\n",
    "        del model_output_tensor\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return embedding_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f23011",
   "metadata": {},
   "source": [
    "## Extract embeddings from ImageNet's images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2eec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images settings\n",
    "img_size = 384\n",
    "\n",
    "# Paths\n",
    "images_folder = os.path.join(\"..\", \"..\", \"CV_data\", \"miniImageNet\")\n",
    "save_path = os.path.join(\"..\", \"..\", \"CV_data\", \"separated_embeddings\", \"qwen_384\")\n",
    "train_images_csv = os.path.join(\"..\", \"assets\", \"train_images.csv\")\n",
    "\n",
    "# Extract list of image names from CSV\n",
    "df = pd.read_csv(train_images_csv)\n",
    "image_names = df['filename'].tolist()\n",
    "\n",
    "# Loop through images and extract embeddings\n",
    "loop = tqdm(image_names, desc=\"[LOOP] Processing images\", unit=\"image\")\n",
    "for file_name in loop:\n",
    "    image_path = os.path.join(images_folder, file_name)\n",
    "    embedding = get_qwen_embedding(image_path, img_size=img_size)\n",
    "\n",
    "    # Save embedding if successfully obtained\n",
    "    if embedding is not None:\n",
    "        torch.save(embedding, os.path.join(save_path, file_name.split(\".\")[0] + \".pt\"))\n",
    "    else:\n",
    "        print(f\"[ERROR] Failed to get embedding for {file_name}, skipping.\")\n",
    "\n",
    "    # Free up memory after processing each image deleting the embedding\n",
    "    del embedding\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_py311 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
